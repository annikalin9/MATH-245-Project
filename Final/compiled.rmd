---
title: "Asteroid Compiled"
author: "Annika Lin"
date: "2023-04-04"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(caret)
library(mlbench)
library(rattle) 
library(randomForest)
library(ROCR)
library(glmnet)
library(car)
library(ResourceSelection)
```

```{r}
#import dataset filtered for '2017-04-06'
nasa <- read.csv("~/Documents/Georgetown/Spring23/Statistical Learning & Data Science/Project/NASA-asteroid-Classification-master/final/nasa.csv")

nasa <- nasa[ , !(names(nasa) %in% c("X"))]

#remove outlier
nasa <- nasa[-695,]
```

```{r}
prop.hazardous <- prop.table(table(nasa$Hazardous))
prop.hazardous
```

```{r}
par(mfrow=c(1,2))
# pie chart
count.hazardous <- table(nasa$Hazardous)
lbls <- paste(levels(as.factor(nasa$Hazardous)), ": ",
              round(prop.hazardous,3)*100, "%", sep="")
pie(count.hazardous, labels=lbls, col=c("salmon", "palegreen"))
# bar plot
barplot(prop.hazardous, xlab="Hazardous", ylab="Proportion", ylim=c(0, 1.0),
        col=c("salmon", "palegreen"))
```

## Analyses of Predictors 

(in search of outliers that might skew analysis)

```{r}
# numerical summary + box plots
summary(nasa)
par(mfrow=c(2,4))
boxplot(nasa$Est.Dia.in.KM.range, main="Est.Dia.in.KM.range")
boxplot(nasa$Relative.Velocity, main="Relative.Velocity")
boxplot(nasa$Minimum.Orbit.Intersection, main="Minimum.Orbit.Intersection")
boxplot(nasa$Inclination, main="Inclination")
boxplot(nasa$Orbital.Period, main="Orbital.Period")
boxplot(nasa$Aphelion.Dist, main="Aphelion.Dist")
boxplot(nasa$Mean.Motion, main="Mean.Motion")
# to find row number of outlier observations
nasa[which.max(nasa$Est.Dia.in.KM.range),]
```

## Predictive Performance Stats

```{r}
get_stats <- function(CM) {
  TP <- CM[2,2]
  FP <- CM[1,2]
  TN <- CM[1,1]
  FN <- CM[2,1]
  
  acc <- (TP+TN) / (TP+TN+FN+FP)
  err <- (FP+FN) / (TP+TN+FN+FP)
  pre <- (TP) / (TP+FP)
  sen <- (TP) / (TP+FN)
  spe <- (TN) / (TN+FP)
  fme <- (2*pre*sen) / (pre+sen)
  mcc_denom <- sqrt(TP+FP)*sqrt(TP+FN)*sqrt(TN+FP)*sqrt(TN+FN)
  mcc <- (TP*TN - FP*FN) / mcc_denom
  
  name <- c("accuracy", "error rate", "precision", "sensitivity", "specificity", "F-measure", "Matthew's CC")
  value <- c(acc, err, pre, sen, spe, fme, mcc)
  stats <- data.frame(name, value)
  
  return (stats)
}
```



# Lassoâ€™s Penalized Regression

Create the design matrix
```{r}
X = model.matrix(Hazardous ~ ., data=nasa) 
Y = as.numeric(nasa$Hazardous=="True")
```

Conduct the cross-validation
```{r}
set.seed(1)
cvfit = cv.glmnet(x=X[,-1], y=Y, family="binomial", type.measure="auc")
cvfit
plot(cvfit)
```

Variables selected using lambda.1se
```{r}
sel.vars <- which(coef(cvfit, s=cvfit$lambda.1se)!=0)[-1]-1
sel.names <- colnames(nasa)[sel.vars]
sel.names
```


```{r}
#fit a lasso model using the selected variables
fit.lasso <- glm(as.factor(Hazardous) ~ Absolute.Magnitude + Est.Dia.in.KM.min. + Orbit.Uncertainity + 
                   Minimum.Orbit.Intersection + Jupiter.Tisserand.Invariant,
                family="binomial", data=nasa)
summary(fit.lasso)
```

```{r}
#address the model assumptions of the lasso model -- multicollinearity
vif(fit.lasso)
```

```{r}
#adjust the model based on multicollinearlity issues
#remove Absolute.Magnitude
fit.lasso2 <- glm(as.factor(Hazardous) ~ Est.Dia.in.KM.min. + Orbit.Uncertainity + 
                   Minimum.Orbit.Intersection + Jupiter.Tisserand.Invariant,
                family="binomial", data=nasa)
summary(fit.lasso2)
```

```{r}
#assess multicollinearity of the adjusted model
vif(fit.lasso2)
```

```{r}
#test the goodness of fit of the adjusted model
hoslem.test(fit.lasso2$y, fit.lasso2$fitted.values)
```

```{r}
#adjust the model based on multicollinearlity issues
#remove Est.Dia.in.KM.min
fit.lasso3 <- glm(as.factor(Hazardous) ~ Absolute.Magnitude + Orbit.Uncertainity + 
                   Minimum.Orbit.Intersection + Jupiter.Tisserand.Invariant,
                family="binomial", data=nasa)
summary(fit.lasso3)
```

```{r}
#assess multicollinearity of the adjusted model
vif(fit.lasso3)
```

```{r}
#test the goodness of fit of the adjusted model
hoslem.test(fit.lasso3$y, fit.lasso3$fitted.values)
```

Fit.lasso3 is the best fit of the lasso model

```{r}
#create the cross-validated model using the selected variables

set.seed(1)
fit.cv <- train(as.factor(Hazardous) ~ Absolute.Magnitude + Est.Dia.in.KM.min. + Orbit.Uncertainity + 
                   Minimum.Orbit.Intersection + Jupiter.Tisserand.Invariant , 
                   method = "glm", family = "binomial", trControl = trainControl(method="cv", number=5, 
                                                savePredictions = TRUE),data=nasa)
fit.cv
#determine the final model
summary(fit.cv$finalModel)
```

```{r}
#asses the mullitcollinearity of the final model
vif(fit.cv$finalModel)
```

```{r}
#create a new cross-validated model removing Est.Dia.in.KM.min.
fit.cv2 <- train(as.factor(Hazardous) ~ Absolute.Magnitude + Orbit.Uncertainity + 
                   Minimum.Orbit.Intersection + Jupiter.Tisserand.Invariant , 
                   method = "glm", family = "binomial", trControl = trainControl(method="cv", number=5, 
                                                                                 classProbs = TRUE,
                                                savePredictions = TRUE),data=nasa)
fit.cv2
#determine the final model
summary(fit.cv2$finalModel)
```

```{r}
#assess the multicollinearity of the adjusted cross-validated model
vif(fit.cv2$finalModel)
```

```{r}
#goodness of fit
hoslem.test(fit.cv2$finalModel$y, fit.cv2$finalModel$fitted.values)
```

```{r}
#assess the predictive performance using the optimized model
pihat <- predict(fit.cv2, type="prob")
head(cbind(nasa$Hazardous, pihat, predict(fit.cv2)))

pred <- prediction(pihat[,2], nasa$Hazardous)
perf <- performance(pred, "tpr", "fpr")
plot(perf)
auc <- performance(pred, "auc")@y.values
auc
```

```{r}
#assess the predictive performance using the predictive model
# pihatcv <- fit.cv2$pred
# head(cbind(nasa$Hazardous[pihatcv$rowIndex], pihatcv))
# predcv <- prediction(pihatcv$True, pihatcv$obs)
# perfcv <- performance(predcv, "tpr", "fpr")
# plot(perfcv)
# auccv <- performance(predcv, "auc")@y.values
# auccv

predprob.lasso <- fit.cv2$pred 
head(predprob.lasso)

pred.lasso = prediction(predprob.lasso$True, predprob.lasso$obs) 
perf.lasso = performance(pred.lasso, "tpr", "fpr") 
plot(perf.lasso)
auc.lasso = performance(pred.lasso, "auc")@y.values
auc.lasso
```

```{r}
confusionMatrix(predprob.lasso$pred, predprob.lasso$obs, positive="True") 
# Confusion matrix
conf.lasso <- table(predprob.lasso$pred, predprob.lasso$obs) 
conf.lasso
```


```{r}
lasso.stats <- get_stats(conf.lasso)
lasso.stats 
```


# k-Nearest Neighbor

1. Make sure that the model assumptions, if any, are satisfied.

KNN makes no assumptions and has relatively few parameters to specify (k and a distance measure).

2. Assess the model fit and perform diagnostics, if appropriate.

```{r}
# function to normalize data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x))) }

arr.norm <- apply(nasa[,-21], 2, normalize)

arr.norm <- data.frame(arr.norm, nasa$Hazardous)

colnames(arr.norm)[colnames(arr.norm) == "nasa.Hazardous"] ="Hazardous"
```

```{r}
# 5-fold CV to choose k

set.seed(1)

arr.norm$Hazardous <- as.factor(arr.norm$Hazardous)

fit.knn <- train(Hazardous ~ .,
  method = "knn",
  tuneGrid = expand.grid(k = 1:21),
  trControl = trainControl(method="cv", number=5, savePredictions = TRUE, classProbs = TRUE),
  metric = "Accuracy",
  data = arr.norm)

fit.knn
```


5 neighbors are used in the final model.

10 most important variables are:
```{r}
imp.knn <- rownames(varImp(fit.knn)$importance)
sel.knn <- imp.knn[order(varImp(fit.knn)$importance[,1], decreasing=T)][1:10] 
sel.knn
```

```{r}
pihatcv.knn <- fit.knn$pred[fit.knn$pred$k == 5,]
pred <- prediction(pihatcv.knn$True, pihatcv.knn$obs)
perf <- performance(pred, "tpr", "fpr")
plot(perf)

# Area under ROC curve (AUC) = concordance index
auc.perf = performance(pred, "auc")
knn_auc <- auc.perf@y.values
knn_auc
```
KNN has an AUC of 0.8553424.
 

```{r}
plot(fit.knn$results[,1], 1-fit.knn$results[,2], type="l",
xlab="k, number of neighbors", ylab="error rate")
```

```{r}
plot(varImp(fit.knn), main="kNN variable importance")
varImp(fit.knn)
```

```{r}
predprob.knn <- fit.knn$pred
predprob.knn <- predprob.knn[predprob.knn$k==as.numeric(fit.knn$bestTune),]

pred.knn = prediction(predprob.knn$True, predprob.knn$obs) 
perf.knn = performance(pred.knn, "tpr", "fpr")
knn.cut <- cbind(unlist(perf.knn@y.values), unlist(perf.knn@x.values), unlist(perf.knn@alpha.values))
knn.cut[knn.cut[,1]>0.7 & knn.cut[,2]<0.3,]
knn.cut[knn.cut[,3] == 0.5,]
confusionMatrix(predprob.knn$pred, predprob.knn$obs, positive="True")


# Confusion matrix
conf.knn <- table(predprob.knn$pred, predprob.knn$obs) 
conf.knn
```

```{r}
knn.stats <- get_stats(conf.knn)
knn.stats 
```


# Classification Tree Analysis

1. Make sure that the model assumptions, if any, are satisfied.

No model assumptions of decision trees to be satisfied? We are fitting a classification tree as opposed to a regression tree because the response variable is categorical and binary.

2. Assess the model fit and perform diagnostics, if appropriate.

Both methods appear to give identical results. Note the identical accuracy and kappa ratings across all cp tuning parameters in the plain rpart method...

```{r}
set.seed(1)
# rpart
nasa.CVrpart <- train(Hazardous ~ ., data=nasa,
                      method="rpart",
                      tuneGrid = expand.grid(cp=seq(0.005, 0.05, length=10)),
                      trControl=trainControl(method="cv", number=5,
                                             savePredictions=TRUE,
                                             classProbs=TRUE,
                                             selectionFunction = "oneSE"))
nasa.CVrpart
# print tree
fancyRpartPlot(nasa.CVrpart$finalModel)
```

3. Identify tuning parameters to be used, if appropriate.

If the plain rpart method is utilized, it selects a cp (complexity parameter) value of 0.05 to maximize accuracy for the final model. Note that the accuracy and kappa values are identical for each of the cp values tried by the model, so any choice within that range should produce comparable results.

4. Identify and interpret the effect of selected variables.

```{r}
# variable importance (rpart)
varImp(nasa.CVrpart)
plot(varImp(nasa.CVrpart))
```

5. Evaluate the cross-validated (CV) predictive performance.

```{r}
head(nasa.CVrpart$pred)
```

```{r}
pihatcv.rpart <- nasa.CVrpart$pred[nasa.CVrpart$pred$cp == 0.05,]
predcv.rpart <- prediction(pihatcv.rpart$True, pihatcv.rpart$obs)
perfcv.rpart <- performance(predcv.rpart, "tpr", "fpr")
plot(perfcv.rpart)
abline(a=0, b=1, lty=2)
aucCV.rpart <- performance(predcv.rpart, "auc")@y.values 
aucCV.rpart
```

```{r}
confMat <- table(pihatcv.rpart$obs, pihatcv.rpart$pred)
confMat
rpart.stats <- get_stats(confMat)
rpart.stats
```

# Random Forest Analysis

1. Make sure that the model assumptions, if any, are satisfied.

No model assumptions of random forest to be satisfied?

2. Assess the model fit and perform diagnostics, if appropriate.

```{r}
set.seed(1)
nasa.rf <- randomForest(as.factor(Hazardous) ~ ., data=nasa)
nasa.rf
```

```{r}
set.seed(1)
nasa.CVrf <- train(Hazardous ~ ., data=nasa,
                   method="rf",
                   trControl=trainControl(method="cv", number=5,
                                          savePredictions=TRUE,
                                          classProbs=TRUE))
nasa.CVrf
```

3. Identify tuning parameters to be used, if appropriate.

The final model selects an mtry tuning parameter value of 20 in order to maximize accuracy.

4. Identify and interpret the effect of selected variables.

```{r}
varImp(nasa.CVrf)
plot(varImp(nasa.CVrf))
```

5. Evaluate the cross-validated (CV) predictive performance.

```{r}
head(nasa.CVrf$pred)
```

```{r}
pihatcv.rf <- nasa.CVrf$pred[nasa.CVrf$pred$mtry == 20,]
predcv.rf <- prediction(pihatcv.rf$True, pihatcv.rf$obs)
perfcv.rf <- performance(predcv.rf, "tpr", "fpr")
plot(perfcv.rf)
abline(a=0, b=1, lty=2)
aucCV.rf <- performance(predcv.rf, "auc")@y.values 
aucCV.rf
```

```{r}
confMat <- table(pihatcv.rf$obs, pihatcv.rf$pred)
confMat
rf.stats <- get_stats(confMat)
rf.stats
```

# Model Comparisons

## variable importance comparison
```{r}
par(mfrow=c(2,2))
plot(varImp(fit.cv2), top=10)
plot(varImp(fit.knn), top=10)
plot(varImp(nasa.CVrpart), top=10)
plot(varImp(nasa.CVrf), top=10)
```

## stats table
```{r}
lasso.stats 

knn.stats 

rpart.stats

rf.stats

df_merge <- merge(lasso.stats,knn.stats,by="name")
colnames(df_merge)[colnames(df_merge) == "value.x"] ="Lasso"
colnames(df_merge)[colnames(df_merge) == "value.y"] ="kNN"

df_merge <- merge(df_merge,rpart.stats,by="name")
df_merge <- merge(df_merge,rf.stats,by="name")
colnames(df_merge)[colnames(df_merge) == "value.x"] ="Classification Tree"
colnames(df_merge)[colnames(df_merge) == "value.y"] ="Random Forest"

df_merge
```

```{r}

imp.knn <- rownames(varImp(fit.knn)$importance)
sel.knn <- imp.knn[order(varImp(fit.knn)$importance[,1], decreasing=T)][1:10]

imp.rp <- rownames(varImp(nasa.CVrpart)$importance)
sel.rp <- imp.rp[order(varImp(nasa.CVrpart)$importance[,1], decreasing=T)][1:10]

imp.rf <- rownames(varImp(nasa.CVrf)$importance)
sel.rf <- imp.rf[order(varImp(nasa.CVrf)$importance[,1], decreasing=T)][1:10]

intersect(sel.names, intersect(sel.knn, intersect(sel.rp, sel.rf)))
```


