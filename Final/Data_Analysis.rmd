---
title: "Asteroid Data Analysis Project"
author: "Annika Lin"
date: "2023-04-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
df <- read.csv("~/Documents/Georgetown/Spring23/Statistical Learning & Data Science/Project/NASA-asteroid-Classification-master/nasa_4_4_23.csv")
df <- df[ , !(names(df) %in% c("X"))]
```

# 1. Lasso-penalized Logistic Regression

### (1.a) Perform lasso variable selection using the area under the curve (AUC) for the receiver operating characteristic (ROC) curve as criterion for choosing the penalty parameter λ. 
Use set.seed(1). List the variables selected using lambda.1se. 
[Note: Do not print out the coefficients of all covariates. Provide only the names of the selected variables.]

```{r}
library(glmnet)
# we use the function model.matrix to create the design matrix
X = model.matrix(Hazardous ~ ., data=df)
Y = as.numeric(df$Hazardous=="True")

set.seed(1)
cvfit = cv.glmnet(x=X[,-1], y=Y, family="binomial", type.measure="auc")
plot(cvfit)
```

```{r}
# coef(cvfit, s=cvfit$lambda.1se)
sel.vars <- which(coef(cvfit, s=cvfit$lambda.1se)!=0)[-1]-1
sel.names <- colnames(df)[sel.vars]
sel.names
```

### (1.b) Fit a 5-fold cross-validated (CV) logistic regression model using the lasso-selected variables with set.seed(1).
```{r}
library(caret)
  # paste(sel.names, collapse = "+")

set.seed(1)
fit.df <- train(Hazardous ~ Absolute.Magnitude+Est.Dia.in.KM.min.+Orbit.Uncertainity+Minimum.Orbit.Intersection+Jupiter.Tisserand.Invariant+Range.Dia.in.KM, method = "glm",
  trControl = trainControl(method="cv", number=5, savePredictions = TRUE),
  data=df)

fit.df
```

**i. Assess if the final model has multicollinearity problems.**
```{r}
summary(fit.df$finalModel)
```
There are no categorical covariates with more than two levels in the model so we use VIF.

```{r}
library(car)
vif(fit.df$finalModel)
```
There does not appear to be an issue of multicollinearity since VIF < 5 for all variables.

ii. Assess the goodness-of-fit of the final model.

```{r}
library(ResourceSelection)
res = hoslem.test(fit.df$finalModel$y, fit.df$finalModel$fitted.values)
res
```
Since there are continuous variables in this model, we use
Hosmer-Lemeshow goodness-of-fit test. With a p-value < 2.2e-16, we reject H0. 
The model does not appear to fit the data well.


iii. Interpret the regression coefficient of the predictor with smallest p-value [Note: the intercept is not a predictor].


Absolute.Magnitude          -3.027e+00  2.275e-01 -13.305  < 2e-16 ***
Est.Dia.in.KM.min.          -5.929e+09  1.826e+09  -3.248  0.00116 ** 
Orbit.Uncertainity          -1.332e-01  4.746e-02  -2.806  0.00502 ** 
Minimum.Orbit.Intersection  -1.227e+02  8.280e+00 -14.815  < 2e-16 ***
Range.Dia.in.KM              4.796e+09  1.477e+09   3.248  0.00116 ** 

Absolute.Magnitude and Minimum.Orbit.Intersection  are significant at the .1% level.


0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

### (1.c) Provide the cross-validated ROC curve and its AUC [Note: you should use the cross-validated prediction to construct the ROC curve].

```{r}
# 
pihat <- predict(fit.df, type="prob") 
# Using cutoff of pi_0=0.5
yhat <- pihat>0.5

library(ROCR)
# Plot ROC curve
pred = prediction(fitted(fit.df), df$Hazardous)
perf = performance(pred, "tpr", "fpr")
plot(perf)
abline(a=0, b=1, lty=2)

```

```{r}
# Area under ROC curve (AUC) = concordance index
auc.perf = performance(pred, "auc")
pen_log_auc <- auc.perf@y.values
pen_log_auc
```

### (1.d) Let π0 be the cut-off for predicting hazard What range of π0 values lead to a true positive rate (TPR) > 0.75 and a false positive rate (FPR) < 0.25?

```{r}
# pi0.cut <- cbind(unlist(perf@y.values),
# unlist(perf@x.values),
# unlist(perf@alpha.values))
# pi0.cut[pi0.cut[,1]>0.75 & pi0.cut[,2]<0.25,]
```

### (1.e) Using a cut-off of π0 = 0.35, calculate the cross-validated misHazardousification error rate and the Matthew correlation coefficient [Note: you should use the cross-validated prediction to calculate these metrics].

```{r}
# library(boot)
# 
# mycost <- function(r, pi = 0) mean(abs(r-pi) > 0.35)
# 
# set.seed(10)
# nrep <- 5
# cv.5foldRep <- sapply(1:nrep, function(i) {cv.err <- cv.glm(df, fit.df$finalModel, mycost, K=5)
# cv.err$delta[1]})
# cv.5foldRep
```


# 2. k-nearest neighbors (kNN)

### (2.a) Process the data using min-max normalization. Show the data for the first 5 covariates in the first
3 subjects before and after normalization.
```{r}
library(caret)
# function to normalize data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x))) }

df[1:3,1:5]
arr.norm <- apply(df[,-21], 2, normalize)
arr.norm[1:3,1:5]


arr.norm <- data.frame(arr.norm, df$Hazardous)
# names(arr.norm) <- names(df[1:5])
colnames(arr.norm)[colnames(arr.norm) == "df.Hazardous"] ="Hazardous"
```


### (2.b) Fit kNN using 5-fold CV over a grid of values between 1 and 21 for the number of neighbors k,
using set.seed(1). How many neighbors are used in the final model?
```{r}
# 5-fold CV to choose k

set.seed(1)

arr.norm$Hazardous <- as.factor(arr.norm$Hazardous)

fit.knn <- train(Hazardous ~ .,
  method = "knn",
  tuneGrid = expand.grid(k = 1:21),
  trControl = trainControl(method="cv", number=5, savePredictions = TRUE, classProbs = TRUE),
  metric = "Accuracy",
  data = arr.norm)

fit.knn
```

13 neighbors are used in the final model.

### (2.c) Which are the 10 most important variables using kNN? Is there any overlap with the variables you
selected using the lasso penalized logistic regression?

```{r}
imp2 <- varImp(fit.knn)$importance
head(imp2[order(-imp2[,2]),,drop=FALSE], 10) 
```


### (2.d) Provide the cross-validated ROC curve and its AUC.

```{r}
pihatfin.knn <- predict(fit.knn, type="prob") 
predfin.knn <- prediction(pihatfin.knn[,2], df$Hazardous)
perffin.knn <- performance(predfin.knn, "tpr", "fpr")
plot(perffin.knn)
abline(a=0, b=1, lty=2)

# Area under ROC curve (AUC) = concordance index
auc.perf = performance(predfin.knn, "auc")
knn_auc <- auc.perf@y.values
knn_auc
```

### (2.e) Let π0 be the cut-off for predicting the risk of collision What range of π0 values lead to a true positive rate (TPR) > 0.70 and a false positive rate (FPR) < 0.30?

```{r}
# pi0.cut <- cbind(unlist(perffin.knn@y.values),
# unlist(perffin.knn@x.values),
# unlist(perffin.knn@alpha.values))
# pi0.cut[pi0.cut[,1]>0.7 & pi0.cut[,2]<0.3,]
```


### (2.f) What TPR and FPR is achieved using π0 = 0.5?
```{r}
# pihat <- predict(fit.knn, type="prob") 
# # Using cutoff of pi_0=0.5
# yhat <- pihat>0.5
# table(yhat, df$Hazardous[fit.knn$pred$rowIndex])
# pi0.cut[pi0.cut[,3]>0.45 & pi0.cut[,3]<0.55,]
```


# 3. Classification tree


### (3.a) Fit a decision tree with 5-fold CV using set.seed(1) and the one-SE rule. Plot the final Classification tree.
```{r}
library(rpart)
set.seed(1)
arr.CVrpart <- train(Hazardous ~ ., data=df,
method="rpart",
tuneGrid = expand.grid(cp = seq(0.005, 0.05, length=10)),
trControl = trainControl(method = "cv", number=5,
savePredictions = TRUE,
selectionFunction = "oneSE") )
arr.CVrpart
```


```{r}
library(rattle)
fancyRpartPlot(arr.CVrpart$finalModel)
```


### (3.b) Which are the 10 most important variables for the Hazardousification tree? Is there any overlap with the variables you selected using the lasso penalized logistic regression?
```{r}
imp3 <- varImp(arr.CVrpart)$importance
head((imp3[order(-imp3$Overall),,drop=FALSE]), 10)
```


### (3.c) Provide the cross-validated ROC curve and its AUC.
```{r}
pihat <- predict(arr.CVrpart, type="prob") 

pred <- prediction(pihat[,2], df$Hazardous)
perf <- performance(pred, "tpr", "fpr")
plot(perf)
abline(a=0, b=1, lty=2)

# Area under ROC curve (AUC) = concordance index
auc.perf = performance(pred, "auc")
cart_auc <- auc.perf@y.values
cart_auc
```


### (3.d) Let π0 be the cut-off for predicting the risk of df. What range of π0 values lead to a true
positive rate (TPR) > 0.70 and a false positive rate (FPR) < 0.30?

```{r}
# pi0.cut <- cbind(unlist(perf@y.values),
# unlist(perf@x.values),
# unlist(perf@alpha.values))
# pi0.cut[pi0.cut[,1]>0.7 & pi0.cut[,2]<0.3,]
```


# 4. Random forest

### (4.a) Fit a random forest with 5-fold CV using set.seed(1) and consider a range of values between 85 and 125 with steps of 10 for mtry, the number of randomly selected variables used at each node splitting. What value of mtry is used in the final model?

```{r}
library(randomForest)
set.seed(1)
arr.RF <- train(Hazardous ~ .,
  method = "rf",
  tuneGrid = expand.grid(mtry=seq(85,125, 10)),
  trControl = trainControl(method="cv", number=5, savePredictions = TRUE, classProbs = TRUE),
  metric = "Accuracy",
  data = df)
arr.RF
```


The final value used for the model was mtry = 85.

### (4.b) Which are the 10 most important variables identified by random forest?

```{r}
imp4 <- varImp(arr.RF)$importance
head((imp4[order(-imp4$Overall),,drop=FALSE]), 10)
```

### (4.c) Provide the cross-validated ROC curve and its AUC.

```{r}
pihat <- predict(arr.RF, type="prob") 

pred <- prediction(pihat[,2], df$Hazardous)
perf <- performance(pred, "tpr", "fpr")
plot(perf)
abline(a=0, b=1, lty=2)

# Area under ROC curve (AUC) = concordance index
auc.perf = performance(pred, "auc")
rf_auc <- auc.perf@y.values
rf_auc
```

### (4.d) Let π0 be the cut-off for predicting the risk of df. What range of π0 values lead to a true positive rate (TPR) > 0.75 and a false positive rate (FPR) < 0.25?

```{r}
# pi0.cut <- cbind(unlist(perf@y.values),
# unlist(perf@x.values),
# unlist(perf@alpha.values))
# pi0.cut[pi0.cut[,1]>0.75 & pi0.cut[,2]<0.25,]
```

# Comparison of models

### Provide a table summarizing the AUC for the cross-validated ROC curve for each of the methods
considered (penalized logistic, PC logistic, kNN, PC kNN, CART, random forest).
```{r}
Method <- c("penalized logistic", "kNN", "CART", "random forest")
auc_score <- c(unlist(pen_log_auc),  unlist(knn_auc), unlist(cart_auc), unlist(rf_auc))

auc_tab <- data.frame(Method, auc_score)
# auc_tab

auc_tab[order(-auc_tab$auc_score),,drop=FALSE]
```


### Which variables are deemed important by the four methods using the covariate data (penalized logistic, kNN, CART, random forest)?
```{r}
imp1 <- varImp(fit.df)$importance



mylist <- list(rownames(head((imp1[order(-imp1$Overall),,drop=FALSE]), 10)),
               rownames(head(imp2[order(-imp2[,2]),,drop=FALSE], 10)),
               rownames(head((imp3[order(-imp3$Overall),,drop=FALSE]), 10)),
               rownames(head((imp4[order(-imp4$Overall),,drop=FALSE]), 10))
               )


mydf <- stack(setNames(mylist, seq_along(mylist)))
mydf$ind <- as.numeric(mydf$ind)


names(mydf) <- c("Variables", "count.in.methods")

mydf[order(-mydf$count.in.methods),,drop=FALSE]
```

